---
title: "Diamonds Group Project 2"
author: "Vinnie Gambotto, Steve Meadows, Sai Sreyaa Krishna Mallemala"
date: "`r Sys.Date()`"
output: 
   html_document:
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: true
    code_folding: hide
    number_sections: true
    css: styles.css
editor_options: 
  markdown: 
    wrap: 72

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Predicting Educational Success

Understanding the factors that contribute to students’ academic success has
become increasingly important, especially as schools, families, and community
partners seek effective ways to support K–12 learners. In this project, we
analyze data from the Parent and Family Involvement (PFI) in Education Survey, a
nationally representative dataset collected by the U.S. Census Bureau, to
explore relationships between family engagement, school choice, and student
academic outcomes.

Our goal is to identify which aspects of family involvement—such as
participation in school activities, communication with teachers, assistance with
homework, and perceptions of school quality—are most strongly associated with
markers of academic success. Using a range of Generalized Linear Models (GLMs)
and classification tools, including logistic regression, multinomial regression,
Poisson regression, and Linear and Quadratic Discriminant Analysis (LDA/QDA), we
aim to build predictive models that highlight the most influential variables.

By applying these statistical learning methods through the tidymodels framework,
we address the broader research question:

**Which family and school-related factors best predict student academic success, and how can these insights guide effective support strategies for K–12 students?**

The final results will be presented in a concise report that summarizes both our
modeling process and our key findings. Ultimately, our analysis is designed to
provide actionable insights for GVSU’s K–12 Connect initiative, helping inform
recommendations that support students, empower families, and strengthen
school–family partnerships.

```{r loading libraries, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(tidymodels)
library(e1071)
library(readxl)
library(dplyr)
library(ggformula)
library(naniar)
library(ggplot2)
library(PCAmixdata)
library(themis)
library(ParBayesianOptimization)
library(yardstick)
library(doParallel)
library(resample)
library(flextable)
library(officer)
library(gt)
library(knitr)
library(glmnet)
library(pROC)
library(discrim)
```


# Data Loading and Preprocessing

We began by loading the 2019 Parent and Family Involvement (PFI) data from the
curated Excel file and standardizing the grade variable (ALLGRADEX) so that
Kindergarten categories were collapsed into a single value and grades 1–12 were
mapped onto a consistent 1–12 scale. From this standardized variable, we
restricted the analytic sample to high school students only (grades 6–12). Next,
we cleaned the outcome variable SEGRADES by treating special codes (-1 and 5) as
missing, dropping cases with missing SEGRADES, and then recoding SEGRADES into a
binary achievement indicator: students reporting mostly A’s/B’s (categories 1–2)
were labeled “high” and those reporting mostly C’s or lower (categories 3–4)
were labeled “low.” Finally, we removed a set of identifying or analytically
unnecessary variables (e.g., ZIP code, date of birth fields, interview
identifiers, and raw grade variables) to produce a streamlined dataset focused
on high school students and a clean binary academic success outcome.


```{r data-loading-preprocessing, include=TRUE, message=FALSE, warning=FALSE}
library(readxl)


# Load and preprocess the data
df_2019 <- read_excel("pfi-data.xlsx", sheet = "curated 2019")

# Standardizing ALLGRADEX for 2019 data
df_2019 <- df_2019 %>%
  mutate(
    ALLGRADEX_2019raw = ALLGRADEX,
    ALLGRADEX_std = case_when(
      ALLGRADEX %in% c(2, 3) ~ 0,                       # both K flavors → Kindergarten
      ALLGRADEX >= 4 & ALLGRADEX <= 15 ~ ALLGRADEX - 3, # 1st–12th → 1–12
      TRUE ~ NA_real_
    )
  )

# Filtering for high school students only
df <- df_2019 %>%
  filter(ALLGRADEX_std >= 6 & ALLGRADEX_std <= 12)


# Handling target variable classes and missing values
df <- df %>%
  # turn -1 and 5 into NA
  mutate(SEGRADES = na_if(SEGRADES, -1),
         SEGRADES = na_if(SEGRADES, 5)) %>%

  # drop rows where SEGRADES is missing
  filter(!is.na(SEGRADES)) %>%

  # create binary target: high vs low
  mutate(
    success = case_when(
      SEGRADES %in% c(1, 2) ~ "high",
      SEGRADES %in% c(3, 4) ~ "low"
    ),
    success = factor(success, levels = c("low", "high"))
  )

# Removing all unnecessary variables
df <- df %>%
  select(-ZIPLOCL, -CDOBMM, -CDOBYY, -BASMID, -ALLGRADEX_std, -MOSTIMPT, -INTNUM, -ALLGRADEX_2019raw, -ALLGRADEX, -SEGRADES, - HHPARN19_BRD)
```

# Exploratory Data Analysis


# LDA
```{r}
levels(df$success)
```

# Choosing predictors for LDA

We are using these predictors as they make sense for grades:

HHPARN19X – number of parents in the household
EDCPUB – public vs other
SCCHOICE – school choice
EINTNET – internet at home
SCHLHRSWK – school hours per week
PARGRADEX – parent education
NUMSIBSX – number of siblings

```{r}
predictor_variables <- c(
  "HHPARN19X",  # number of parents in household
  "EDCPUB",     # public vs other school
  "SCCHOICE",   # school choice
  "EINTNET",    # internet at home
  "SCHLHRSWK",  # school hours per week
  "PARGRADEX",  # parent education
  "NUMSIBSX"    # number of siblings
)
```


```{r}
df_model <- df %>%
select(success, all_of(predictor_variables))
```

This step updates the dataset to keep only the response variable success along
with the seven predictors that will be used in the LDA model. These predictors
are for family structure, school type, school choice, internet access, school
hours, parent education, and number of siblings.

# Checking whether all variables are numeric

```{r}
df_model %>% 
  summarise(across(all_of(predictor_variables), ~ class(.)))
```

# Test-Train Data

```{r}
set.seed(123)

data_split <- initial_split(df_model, prop = 0.8, strata = success)
train_data <- training(data_split)
test_data  <- testing(data_split)
nrow(train_data)
nrow(test_data)
```

Stratified sampling based on the success variable is used to divide the data into
an 80% training set and a 20% test set, preserving the same proportion of low and
high success cases in both sets. This results in 7,472 observations for the training
set and 1,869 observations for the test set. We train the LDA model on the training
portion and evaluate the performance of the model using the test portion.


# Fitting the LDA Model

```{r}
lda_model <- discrim_linear() %>%
  set_mode("classification") %>%
  set_engine("MASS", prior = c(low = 0.5, high = 0.5)) %>%
  fit(
    success ~ HHPARN19X + EDCPUB + SCCHOICE +
      EINTNET + SCHLHRSWK + PARGRADEX + NUMSIBSX,
    data = train_data
  )

lda_model
```

The LDA model is fit using equal class priors of 0.5 for low success and 0.5 for
high success, giving equal weight to both groups during training instead of letting
the model automatically favor the majority high-success class. This goes in line
with our goal of increasing the detection of the low-success students. These priors
are shown at the top of the output, followed by the means of each group: for example,
low-success students average values like HHPARN19X = 1.786, EINTNET = 3.872,
and PARGRADEX = 3.126, while high-success students have slightly higher values
such as EINTNET = 3.910, SCHLHRSMK = 3.776, and PARGRADEX = 3.676. These differences
represent how high academic resources and strong parental backgrounds are associated
with high success, and how low values on these variables are associated with 
low success-more generally, information the model then uses to predict which students are at risk. 

The discriminant coefficients further show which predictors pull a student toward
the "low" classification: negative coefficients such as HHPARN19X (-0.705) and 
SCCHOICE (-0.590) push the model toward predicting "low success," whereas positive
coefficients such as EDCPUB (0.681), EINTNET (0.310), and PARGRADEX (0.467) shift
the predictions toward "high success." Together, these numerical results show how
the model combines predictor patterns to distinguish the students, including specific
variables that serve to help highlight those more likely to fall into the low-success category,
which meets the main objective of the analysis.

# Getting predictions on test data 

```{r}
lda_test <- augment(lda_model, new_data = test_data)
head(lda_test)
```


```{r}
conf_mat(lda_test, truth = success, estimate = .pred_class)
```

```{r}
library(ggplot2)
library(dplyr)

cm <- conf_mat(lda_test, truth = success, estimate = .pred_class)

cm_df <- as.data.frame(cm$table)

ggplot(cm_df, aes(x = Prediction, y = Truth, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq), color = "white", size = 6) +
  scale_fill_gradient(low = "lightblue", high = "darkblue") +
  labs(title = "Confusion Matrix Heatmap") +
  theme_minimal()
```

```{r}
metrics <- bind_rows(
  accuracy(lda_test, truth = success, estimate = .pred_class),
  precision(lda_test, truth = success, estimate = .pred_class, event_level = "first"),
  recall(lda_test, truth = success, estimate = .pred_class, event_level = "first"),
  f_meas(lda_test, truth = success, estimate = .pred_class, event_level = "first"),
  roc_auc(lda_test, truth = success, .pred_low)
)

metrics
```
The confusion matrix indicates that this model continues to do well at predicting
the high-success students, correctly identifying 1092 of them, but it continues to
show weak performance for the low-success group. Of the 247 actual low-success students,
only 103 were correctly identified as low, but 144 were misclassified as high,
indicating that a large portion of the students that need attention are still missed.
This trend is mirrored in the evaluation metrics: the precision for the low-success
class is 0.21, which means only 21 percent of the students predicted as low are truly
low, while the recall is 0.58, showing that the model captures a little more than half
of the actual low-success students. Confirming this finding, the F1-score, balancing
precision and recall, is 0.31 and thus again shows the model's poor ability to reliably
flag low-performing students. Even though the overall accuracy is 66.13 percent, this
is driven mainly by strong performance on the high-success group and does not reflect
meaningful progress toward identifying low-success students. At the same time, the
ROC AUC value of 0.67 indicates only moderate ability to correctly rank students
based on their likelihood of having low success. In all, results indicate that even
though the model may perform appropriately for high-success students, it is pretty
limited with respect to detecting low-success students, which is actually the main
challenge for this analysis.

```{r}
library(dplyr)
library(ggplot2)

# 1. Getting the underlying MASS::lda object from the parsnip model
lda_engine <- lda_model$fit

# 2. Predicting discriminant scores on the test data
lda_scores <- predict(lda_engine, newdata = test_data)

# 3. Add LD1 (first discriminant) as a new column to test_data
lda_values <- test_data %>%
  mutate(LD1 = lda_scores$x[, 1])

# 4. Plot LD1 distribution by success group
ggplot(lda_values, aes(x = LD1, fill = success)) +
  geom_density(alpha = 0.4) +
  labs(
    title = "LD1 Distribution by Success Group",
    x = "Linear Discriminant (LD1)",
    fill = "Success"
  ) +
  theme_minimal()

```
It is apparent that the LD1 distribution plot has shown us how well our LDA model
separates the low success students from the high success ones, which is the whole
point of our analysis. Low success students cluster around the left-hand side of
the graph, with lower LD1 values, while high success students move to the right,
where the LD1 values are higher. That means the model is learning some difference
between the two groups. Still, there is a significant overlap between the two curves
in the middle, and this directly explains why the model struggles to correctly
identify low success students: many of the low success students fall within the
same LD1 value range as that of high success students, so the model cannot confidently
distinguish them. Even though the separation trend can be seen, the overlapping
area shows that LD1 alone is not strong enough to isolate the low performing students,
and thus the recall for the low group remains low. This visualization confirms
that the model naturally prefers the high success group and reinforces our
conclusion that identifying low success students remains difficult under this LDA approach.

```{r}
library(ggplot2)

# ROC curve data
roc_data <- roc_curve(lda_test, truth = success, .pred_low)


ggplot(roc_data, aes(x = 1 - specificity, y = sensitivity)) +
geom_line(color="darkgreen", size=1.2) +
geom_abline(linetype="dashed", color="red") +
labs(title="ROC Curve for LOW Success Detection",
x="False Positive Rate",
y="True Positive Rate") +
theme_minimal()

```

The ROC curve shows how well the model can detect low-success students by comparing
the true positive rate to the false positive rate across different thresholds. 
It rises above the diagonal line, which means the model does better than random
guessing but not by a large margin. The curving of this ROC is moderate, matching
the ROC AUC of about 0.67. That indicates the model has only limited ability in 
distinguishing between low-success and high-success students. Sometimes it picks up
the signal for the low group, but not strongly or consistently. Overall, 
the ROC curve confirms that detecting low-success students remains challenging,
directly related to our project goal of early identification of low performers.


#Conclusion

Overall, the LDA model has obvious limitations in reaching the main goal of
identifying the students with low success. Whereas the model performs very well 
in high-success cases-it correctly classifies 1,092 high-success students-it is
still struggling with the group of low success. Among the 247 actual low-success
students, it identified only 103 correctly and misclassified 144 as high, meaning
that more than half of the low performers were missed. This is also reflected in
the evaluation metrics: the precision for low success is only 0.21, while the recall 
is higher, 0.58, but the F1-score is still low at 0.31, confirming weak reliability
in detecting low-performing students. The LD1 distribution plot further shows a big
overlap between the low and high success students, which means the model cannot form
a clear linear boundary to separate the two groups. The AUC of ROC is 0.67, which
means the model only has a moderate ability to distinguish the low-success students
from the high-success students. These results taken together demonstrate that the
model is biased heavily towards high-success predictions and yet misses many students
in need of academic support, which means this LDA approach does not adequately meet
the project's objective of early identification of low-performing students. So, we tried 
other models further.


# Lasso Logistic Regression
To try a logistic regression method for our Predicting Educational Success, we decided to use the LASSO regression method with our response variable still being SEGRADES. We decided to remove any rows that are missing to keep a better data set as it was large enough without including the missing variables. We continued with the categorical response of High grade earners being those who had received mostly A's and B's and our Low grade earners, those with C's or D's. 

We chose to do LASSO regression because of how large the raw data set is. With LASSO we can quickly eliminate any predictors that have little to no use for our model or the question we are trying to solve. And with it, our issues with over fitting and multicollinearity are automatically solved within our program and we do not have to do any VIF checks or worry about including too many or too little predictors. 


```{r}
df <- df %>%
  mutate(
    across(
      .cols = where(is.numeric),
      .fns = ~ na_if(.x , -1)
    )
  )
```

```{r}
df_na_count <- df %>%
  filter(if_any(everything(), is.na)) %>%  
  summarise(rows_with_na = n())            

kable(df_na_count, caption = "Number of Rows with Any Missing Values")
```

```{r}
df<-df %>% drop_na()
```

```{r}
na_count <- df %>%
  filter(if_any(everything(), is.na)) %>%
  summarise(rows_with_na = n())


kable(na_count, caption = "Number of Rows with Any Missing Values")

```

Separate our numerical variables from the rest of our data to make cleaning out the -1, 0, and 5 values with SEGRADES easier.
```{r}
num_vars <- c(
  "FHWKHRS", 
  "FSFREQ",
  "HHTOTALXX",
  "P1HRSWK",
  "P1MTHSWRK",
  "P1AGE",
  "NUMSIBSX"
)
```

```{r}
df <- df %>%
  mutate(
    across(
      .cols = where(is.numeric) & !all_of(num_vars),  
      .fns = ~ ifelse(.x < 0, 0, .x)                
    )
  )

```

Separate into our test and training set. We will use the training set to run our model. From there we will use the test data for our evaluation. We will then see how well our testing set does later on with this model.
```{r}
set.seed(1986)

data_split <- initial_split(df, prop = 0.8, strata = success)
train <- training(data_split)
test <- testing(data_split)
```

```{r}
train_x <- train %>% select(-success)
test_x  <- test  %>% select(-success)
train_x_matrix <- model.matrix(~ . - success - 1, data = train)
test_x_matrix  <- model.matrix(~ . - success - 1, data = test)

train_y <- train$success
test_y <- test$success

```

#Building our LASSO model. 

```{r}
lasso.mod <- glmnet(
  x = train_x_matrix,
  y = train_y,
  alpha = 1, 
  family = "binomial"
)
```

```{r}
plot(lasso.mod)
title("LASSO Coefficient Paths")
```

Here we can see how each predictor’s coefficient changes as the penalty λ changes. It visualizes variable selection and shrinkage, letting you see which predictors enter the model and when. There are about 6 variables that enter the model early and the rest come around 4.5 for -Log(λ)
```{r}
set.seed(1986)

cv.lasso <- cv.glmnet(
  x = train_x_matrix,
  y = train_y,
  alpha = 1,
  family = "binomial",
  type.measure = "class"
)

plot(cv.lasso)
title("LASSO Cross-Validation Curve")

```

Above the plot is suggesting about 22 variables as we see the second dash line crossing right there with the MSE's. Right around that point, we have a -Log(λ) of between 3 and 5. And below we see that our best λ value is very small at 0.00841625 suggesting that the more predictors we keep the better our prediction model will be. 
```{r}
best_lambda <- cv.lasso$lambda.min
kable(best_lambda)

```

```{r}
lasso.final <- glmnet(
  x = train_x_matrix,
  y = train_y,
  alpha = 1,
  lambda = best_lambda,
  family = "binomial"
)

coef_final <- coef(lasso.final)

```
This does work nicely with our MSE plot above as this also concludes with the idea of 22 variables. And the MSE of the 22 variables are better shown below in the table below.


```{r}
coef_df <- as.data.frame(as.matrix(coef_final)) %>%
  rownames_to_column("variable") %>%
  rename(coef = s0) %>%
  filter(coef != 0)
kable(coef_df)

```

```{r}
lasso.prob <- predict(cv.lasso, newx = test_x_matrix, s = "lambda.min", type = "response")
lasso.pred <- ifelse(lasso.prob > 0.5, "high", "low") %>% factor(levels = c("low", "high"))
mean(lasso.pred == test_y)

```

Our training model gave us a 91.6% correct classification rate with our test data set, suggesting a very strong model that is consistent and reproducible in identifying true lows and false highs. 

```{r}
lasso_table<-table(Predicted = lasso.pred, Actual = test_y)

```

```{r}
lasso_table<-as.data.frame(lasso_table)
ggplot(lasso_table, aes(x = Predicted, y = Actual, fill = Freq)) +
  geom_tile() +
  geom_text(aes(label = Freq)) +
  scale_fill_gradient(low = 'yellow', high = 'orange') +
  labs(title = "Confusion Heatmap") +
  theme_minimal()
```


The confusion matrix displays predicted classes on the rows and true classes on the columns. With this table we can see the total number of true positives, true negatives, false positives, and false negatives. Our true positive consisted of 1280 observations, our true negative had 51 total observations, while we had a false positive of 6 and a false negative of 116. Below we can see the metrics that describe how effective our model was:
Recall (True negative (Low) + false negative (Low)) = 51 / 51+6 = 89.47%
Precision (Our predicted negatives or Low's) = 51 / 51+116 = 30.54%
Overall Accuracy = 51+1280/1453 = 91.6%

Our main priority is looking at our "false positives", meaning the students we predicted to be high and who are actually low. This is an issue because with our model if we predict someone to be getting high grades and they are actually getting low they would not be getting the help they need. We are not so worried about false negatives as those students are predicted low who are actually high will just get more help and will do better thus furthering themselves away from being predicted as low. So, with our model there is the error of 116 predicted high students who were actually in the low group who would not receive the necessary help.

```{r}
roc_obj <- roc(test_y, as.numeric(lasso.prob))
plot(roc_obj)
auc(roc_obj)

```

In conclusion, to evaluate the model’s ability, we plotted the ROC curve using the predicted probabilities from the LASSO model. The ROC curve illustrates difference between our true identification rate for Low and false identification rate for those who are Low and are identified as High (False High) across different classification thresholds. The Area Under the Curve (AUC) was 0.9337. This high of value shows that our model performed very well as it suggests that the model has distinguished between true lows and false highs. This high AUC demonstrates that the LASSO model is effective at ranking students by their probability of academic success, making it a reliable tool for identifying students who may need additional support.

# Support Vector Classification

After exploring LDA and Lasso Logistic Regression models that we examined in
class, we also wanted to implement a Support Vector Classifier (SVC) model to
further investigate the predictive power of our features on student academic
success. SVC is particularly useful for handling high-dimensional data and can
effectively capture complex relationships between variables, and we wanted to
experiment with its application in this context to see if it could yield better
classification performance compared to the statistical learning models we
tested.

We created a pipeline that includes Dimensionality Reduction using the
`PCAmixdata` package, followed by SVC modeling using the `e1071` package used in
the textbook. We tuned hyperparameters such as the cost parameter (C) and kernel
using cross-validation and parallel baysian optimization to identify the best
model using PR-AUC on the minority class as the evaluation metric using the
`ParBayesianOptimization` package.

## Data Preparation for PCAmix

Prior to fitting the PCAmix model, we undertook several preprocessing steps to
ensure compatibility with mixed-data factor extraction. In the PFI dataset, the
value `-1` indicates a *valid skip* rather than a substantive numeric response.
Because PCAmix replaces missing values in quantitative variables with the column
mean, retaining `-1` would introduce artificial values into the covariance
structure. We therefore recoded all `-1` entries in numeric variables as `NA`.

For qualitative variables, PCAmix operates on a disjunctive (indicator) matrix
in which missing entries are replaced with zeros—an interpretation consistent
with the absence of a selected response category in valid-skip cases.
Given that the dataset includes both continuous measures (e.g., household size,
work hours, parent age) and categorical survey responses, PCAmix is an
appropriate framework, as it integrates PCA for quantitative variables with an
MCA-like treatment for qualitative variables within a unified component
solution. To support this structure, we explicitly identified the small set of
metrically continuous variables and converted all remaining predictors to
factors.

After generating stratified training and test splits on the binary success
outcome, we removed the outcome variable and applied `splitmix()` to decompose
the predictor set into quantitative (`X.quanti`) and qualitative (`X.quali`)
blocks. This separation is required because PCAmix applies distinct mathematical
transformations to numeric and categorical variables, producing interpretable
mixed-data components suitable for downstream predictive modeling.

```{r data prep for PCAmix, include=TRUE, eval=FALSE}
df <- df %>%
  mutate(
    across(
      .cols = where(is.numeric),
      .fns  = ~ na_if(.x, -1)
    )
  )

num_vars <- c(
  "FHWKHRS",    # hours parent helps with homework per week
  "FSFREQ",     # frequency count of parent participation
  "HHTOTALXX",  # household size
  "P1HRSWK",    # parent's usual weekly work hours
  "P1MTHSWRK",  # months worked last year
  "P1AGE",      # parent's age
  "NUMSIBSX"    # number of siblings
)

# Convert all *non-numeric predictors* (and not success) to factors
df <- df %>%
  mutate(
    across(
      .cols = -c(all_of(num_vars), success),
      .fns  = as.factor
    )
  )

# test/train split
set.seed(1986)

data_split <- initial_split(df, prop = 0.8, strata = success)
train_data <- training(data_split)
test_data  <- testing(data_split)

# drop the outcome for PCAmix
train_x <- train_data %>% select(-success)
test_x  <- test_data  %>% select(-success)

# response vectors
train_y <- train_data$success
test_y  <- test_data$success

# training
sm_train <- splitmix(train_x)
X.quanti_train <- sm_train$X.quanti   
X.quali_train  <- sm_train$X.quali  

# testing
sm_test <- splitmix(test_x)
X.quanti_test <- sm_test$X.quanti  
X.quali_test  <- sm_test$X.quali 
```

## Scree Plot for PCAmix Components

To evaluate the dimensional structure of the mixed data, we fit a PCAmix model
using all available predictors in the training set and extracted the associated
eigenvalues. Because PCAmix yields a component for each input variable, we
examined only the first 30 components for interpretability. A scree plot was
generated to visualize the rate at which variance declines across components and
to identify potential elbows or diminishing returns in explained variance. This
diagnostic informed the subsequent decision on how many components to retain for
rotation and downstream modeling.

![](pcamix_scree_plot.png)

```{r PCAmix scree plot, include=TRUE, eval=FALSE}
pcamix_train <- PCAmix(
  X.quanti = X.quanti_train,
  X.quali  = X.quali_train,
  ndim     = ncol(train_x),
  rename.level = TRUE,
  graph = FALSE
)

k <- 30
eig <- pcamix_train$eig[1:k, ]

eig_df <- data.frame(
  Dim = 1:k,
  Eigenvalue = eig[, 1]
)

ggplot(eig_df, aes(x = Dim, y = Eigenvalue)) +
  geom_point() +
  geom_line() +
  theme_minimal(base_size = 14) +
  labs(
    title = "PCAmix Scree Plot (First 30 Components)",
    x = "Component",
    y = "Eigenvalue"
  )
```

## Selection of 10 Principal Components

Component retention was guided by inspection of the PCAmix scree plot and the
corresponding eigenvalues. The first several components show a steep decline in
eigenvalue magnitude, with Components 1–10 all exceeding approximately 1.9 and
capturing substantively meaningful variation. Beyond the tenth component, the
eigenvalues flatten markedly, ranging narrowly between 1.86 and 1.20 through
Component 30. This gradual, nearly linear decrease indicates the onset of the
“long tail” region, where additional components contribute relatively little
incremental explanatory power.

Although traditional rules such as Kaiser’s criterion (retaining components with
eigenvalues > 1) would suggest keeping many more than ten components in this
mixed-data setting, such heuristics are known to over-retain when the variable
count is large and includes categorical indicators, as in PCAmix. Instead, we
prioritized components that (a) lie above the clear elbow in the scree plot, (b)
exhibit noticeably larger eigenvalue magnitude, and (c) are likely to support
interpretable rotated factors. Under these criteria, a 10-component solution
represents a balance between parsimony and fidelity, preserving the major
dimensions of structure while avoiding inflation of weak, noise-driven
components. This 10-PC solution was therefore used for rotation, interpretation,
and downstream predictive modeling.

## Fitting PCAmix and SVC with Bayesian Hyperparameter Optimization

```{r Fitting PCAmix for 10 components, include=FALSE, eval=FALSE}
## ---- PCAmix with 10 components + Varimax rotation -------------------------
# 1. Fit PCAmix with 10 components
pcamix_train_10 <- PCAmix(
  X.quanti = X.quanti_train,
  X.quali  = X.quali_train,
  ndim = 10,
  rename.level = TRUE,
  graph = FALSE
)

# 2. TRAIN SCORES (10 PCs)
train_scores_10 <- pcamix_train_10$ind$coord
train_scores_10_df <- as.data.frame(train_scores_10)
colnames(train_scores_10_df) <- paste0("PC", 1:10)
train_scores_10_df$success <- train_y

# 3. TEST SCORES
test_scores_10 <- predict(
  pcamix_train_10,
  X.quanti = X.quanti_test,
  X.quali  = X.quali_test,
  rename.level = TRUE
)
test_scores_10_df <- as.data.frame(test_scores_10)
colnames(test_scores_10_df) <- paste0("PC", 1:10)
test_scores_10_df$success <- test_y
```


```{r Bayesian optimisation for SVC, include=TRUE, eval=FALSE}
## ---- bayesian-optimisation -------------------------------------------------
# 1. Packages ---------------------------------------------------------
library(ParBayesianOptimization)   # bayesOpt
library(e1071)                      # svm
library(rsample)                    # vfold_cv  <-- THIS IS THE FIX
library(yardstick)                  # pr_auc_vec
library(dplyr)                      # mutate, map, etc.
library(doParallel)                 # parallel backend

# 2. Parallel set-up --------------------------------------------------
cl <- makeCluster(detectCores() - 1)
registerDoParallel(cl)

# Export data + objects
clusterExport(cl, varlist = c("train_scores_10_df", "class_weights"))

# Load required packages on every worker
clusterEvalQ(cl, {
  library(e1071)
  library(rsample)      # <-- vfold_cv lives here
  library(yardstick)
  library(dplyr)
})

# 3. Scoring function (5-fold CV, PR-AUC) -----------------------------
svm_cv_score <- function(cost, gamma) {
  # Guard against illegal values
  if (cost <= 0 || gamma <= 0) return(list(Score = -Inf))

  tryCatch({
    set.seed(123)                                   # reproducible folds
    folds <- vfold_cv(train_scores_10_df, v = 5, strata = success)

    pr_vals <- numeric(nrow(folds))
    for (i in seq_len(nrow(folds))) {
      split    <- folds$splits[[i]]
      train_i  <- analysis(split)
      test_i   <- assessment(split)

      mod <- svm(
        success ~ .,
        data = train_i,
        kernel = "radial",
        cost = cost,
        gamma = gamma,
        class.weights = class_weights,
        probability = TRUE
      )

      pred <- predict(mod, test_i, probability = TRUE)
      prob_low <- attr(pred, "probabilities")[, "low"]

      pr_vals[i] <- pr_auc_vec(test_i$success, prob_low,
                               event_level = "first")
    }

    list(Score = mean(pr_vals, na.rm = TRUE))
  }, error = function(e) {
    list(Score = -Inf)      # any crash → terrible score
  })
}

# 4. Bounds -----------------------------------------------------------
bounds <- list(
  cost  = c(0.1, 50),
  gamma = c(0.01, 2)
)

# 5. Bayesian optimisation -------------------------------------------
set.seed(123)

opt <- bayesOpt(
  FUN = svm_cv_score,
  bounds = bounds,
  initPoints = 10,
  iters.n = 30,
  iters.k = 4,
  acq = "ei",
  kappa = 2.576,
  eps = 0,
  parallel = TRUE,
  gsPoints = 200,
  acqThresh = 0.9,
  errorHandling = 10,          # allow up to 10 failures
  plotProgress = TRUE,
  saveFile = "svm_bayes_opt.RDS",
  verbose = 2
)

stopCluster(cl)   # clean-up

# 6. Best parameters --------------------------------------------------
best <- getBestPars(opt)
cat(sprintf("Best cost = %.3f, gamma = %.4f, CV PR-AUC = %.4f\n",
            best$cost, best$gamma, max(opt$scoreSummary$Score)))

# 7. Final model on the *full* training set ---------------------------
final_svm_10 <- svm(
  success ~ .,
  data = train_scores_10_df,
  kernel = "radial",
  cost = best$cost,
  gamma = best$gamma,
  class.weights = class_weights,
  probability = TRUE
)

# 8. Test PR-AUC ------------------------------------------------------
test_pred <- predict(final_svm_10, test_scores_10_df, probability = TRUE)
test_prob <- attr(test_pred, "probabilities")[, "low"]

test_pr_auc <- pr_auc_vec(test_scores_10_df$success, test_prob,
                          event_level = "first")
cat("Test PR-AUC =", test_pr_auc, "\n")
```

To tune the radial-basis SVM classifier, we optimized two key hyperparameters:
**cost** and **gamma**. The *cost* parameter regulates the penalty assigned to
misclassified observations, where smaller values allow a softer and more
flexible margin, while larger values enforce stricter separation that can lead
to overfitting. The *gamma* parameter controls the width of the radial basis
function kernel. Low gamma values generate smoother, more global decision
boundaries, whereas high gamma values produce highly localized boundaries that
risk capturing noise in the training set.

Because the dataset is strongly imbalanced—with *low-success* students being far
less frequent than *high-success* students—we incorporated **class weights**
directly in the SVM optimization. We computed weights proportional to the
inverse class frequencies:

- **low**: 6.0256
- **high**: 1.0000

This weighting scheme forces the model to treat misclassification of the
minority *low* class as roughly six times more costly than misclassification of
a *high* case. Incorporating class weights prevents the classifier from
defaulting to the majority class and ensures that the decision boundary is
shaped to improve detection of the students most relevant to our prediction
objective.

Hyperparameter performance was evaluated using **five-fold stratified
cross-validation**, preserving the proportion of low and high cases in each
fold. For each (cost, gamma) pair proposed by the Bayesian optimizer, the SVM
was trained on four folds and evaluated on the remaining fold. Performance was
quantified using the **Precision–Recall Area Under the Curve (PR-AUC)** computed
for the *low* class. PR-AUC was chosen instead of ROC-AUC because the outcome is
imbalanced: PR-AUC more directly measures how well the classifier identifies and
ranks the minority group, which is the primary target for early detection and
intervention.

Bayesian optimization (using an Expected Improvement acquisition function)
efficiently searched the hyperparameter space, balancing exploration of
uncertain regions and exploitation of promising ones. The process converged on a
well-regularized configuration: **cost = 0.10** and **gamma = 0.01**, yielding a
**cross-validated PR-AUC of 0.5811**. Training a final model on the full
training set with these tuned parameters produced a **test-set PR-AUC of
0.6340**, indicating improved generalization and effective discrimination of
low-success students despite substantial class imbalance.

## Relationship Between Hyperparameters and Model Performance

![](cost_vs_score.png)

![](gamma_vs_score.png)

The scatter plots of PR-AUC against **cost** and **gamma** reveal a clear
structural pattern in how the SVM responds to regularization. Models achieve
their strongest performance when **cost is small** and **gamma is extremely
small**, indicating that the classifier benefits from a **soft margin** and a
**very smooth, broad RBF kernel**. As either hyperparameter increases—especially
when gamma becomes moderate or large—the PR-AUC drops sharply. This pattern
shows that highly flexible or highly localized decision boundaries (high gamma
or high cost) tend to overfit the majority class and degrade ranking performance
for the minority *low-success* group. Conversely, the highest PR-AUC values
cluster in the region where the boundary is simple and heavily regularized,
consistent with the final optimal configuration (cost = 0.10, gamma = 0.01).

## SVM Confusion Matrix on Test Data

```{r SVM Confusion Matrix, include=FALSE, eval=FALSE}
# Add predictions to test data
test_scores_10_df$pred <- predict(final_svm_10, test_scores_10_df)

# Confusion matrix plot
test_scores_10_df %>%
  conf_mat(truth = success, estimate = pred) %>%
  autoplot(type = "heatmap") +
  scale_fill_gradient(low = "white", high = "steelblue") +
  labs(title = "SVM Confusion Matrix (Test Data)",
       subtitle = "Recall(low) = 89.5%, Precision(low) = 40.8%") +
  theme_minimal()
```

![](svm_confusion_matrix.png)

The confusion matrix displays predicted classes on the rows and true classes on
the columns. Thus, each cell reflects how often the model assigned a given label
relative to the actual outcome.

- **Predicted low / True low (TP)**: 221
- **Predicted low / True high (FP)**: 320
- **Predicted high / True low (FN)**: 26
- **Predicted high / True high (TN)**: 1302

Interpreting *low* as the at-risk (positive) class:

- **Recall (sensitivity)** = 221 / (221 + 26) ≈ **89.5%**  
  - The model correctly identifies most low-success students and rarely misses them.
- **Precision** = 221 / (221 + 320) ≈ **40.8%**  
  - Of all students predicted to be low-success, only about 41% actually are; the remainder are false alarms.
- **Overall accuracy** ≈ (221 + 1302) / (221 + 320 + 26 + 1302) ≈ **81.5%**

![](precision_recall_curve.png)

The precision–recall curve illustrates how the model balances correctly
identifying low-success students (recall) against the proportion of flagged
students who are truly low-success (precision). The model achieves strong recall
across most thresholds, reflecting its ability to detect the majority of at-risk
students, while maintaining precision well above the random-chance baseline of
14%. The resulting PR-AUC of 0.634 indicates substantially better minority-class
ranking performance than would be expected by chance.

These results indicate that the model is optimized toward **high recall for the
low-success group**, which aligns with the project goal of minimizing false
negatives (i.e., failing to detect at-risk students). The tradeoff is a higher
false-positive rate, meaning some high-success students are flagged as low. This
pattern is consistent with the class weighting and the PR-AUC–driven tuning
procedure, which prioritize identifying as many at-risk students as possible.

Importantly, in an educational context, the “cost” of these false positives may
not be strictly negative. Students incorrectly flagged as low-success may still
benefit from additional academic support, mentoring, or resource allocation. Such
interventions can reinforce positive behaviors, bolster engagement, and help
counteract patterns associated with multigenerational academic disadvantage. Thus,
while the model errs on the side of over-identification, this may serve a broader
equity-oriented purpose by expanding access to supportive structures that promote
long-term success.

## Comparison of Alternative SVM Models

To assess the stability of predictive performance across different levels of
dimensionality reduction, we estimated three SVM models using PCAmix solutions
with 10, 5, and 3 principal components. Across all metrics, the **10-component
model** demonstrated the strongest performance. It achieved the highest
cross-validated PR-AUC (0.581), the highest test PR-AUC (0.634), and the highest
ROC-AUC (0.924). This model also delivered the strongest recall for the
low-success group (0.895), indicating that it most effectively identifies
at-risk students. Although it produced more false positives than the more
compact models, it maintained the lowest number of false negatives—consistent
with its design objective of maximizing early detection.

The **5-component model** exhibited moderate performance, with a test PR-AUC of
0.517 and a noticeable decline in ROC-AUC and overall accuracy. Its recall for
the low-success group dropped to 0.814, and the number of false negatives nearly
doubled relative to the 10-component model. Despite flagging more students
overall, its precision remained lower, suggesting weaker minority-class ranking
ability.

The **3-component model** performed the weakest of the three. With a test PR-AUC
of 0.475 and the lowest ROC-AUC (0.849), this model struggled to separate low-
and high-success students. Recall fell to 0.785, and false negatives increased
to 53. Although it produced fewer false positives than the 5-component model,
its lower discriminative performance suggests that substantial predictive signal
was lost when reducing the dimensionality to three components.
Overall, the comparison indicates that reducing the PCAmix representation too
aggressively degrades minority-class detection and ranking quality. The
**10-component model** offers the most balanced and effective performance,
preserving enough mixed-data structure to support high recall and strong PR-AUC
while maintaining acceptable precision and accuracy.

![](svm_model_comparison.png)


# Model Selection

After completing three separate modeling approaches (LDA, LASSO Logistic Regression, and PCAmix) we observed clear differences in predictive performance, with our main focus being on the identification of low students.

LDA did decently well for the majority of high students (overall accuracy ≈ 66%) but struggled with low students (recall =58%, precision =21%, F1-Score =0.31). The LDA distributions showed significant overlap, showing how low-performing students were misclassified, which could pose issues as students who may need the extra help would not receive it.

LASSO Logistic Regression appeared to have the best overall predictive ability, with 91.6% accuracy and an AUC of 0.9337. While recall for the low-success class was high (89.47), an ok precision score (30.54%), and had an excellent overall accuracy (91.6%) the model still misclassified some low-success students, though far fewer than LDA. Which is bound to happen as no model ever has been 100% completely perfect without multicollinearity and overfitting the data.

PCAmix achieved a high recall (=89.5%) for low-success students, and had a good precision score (=40.8%) and overall accuracy (=81.5%). The PR-AUC (0.634) showed strong low ranking performance, and the precision–recall curve shows the model’s capacity to detect at-risk low students consistently.

Overall, if our goal is to identify those students who are in desperate need of help (Low students), we will want to pick the model that best predicts those students who are low and are predicted low at a high rate, but more importantly having a low rate of students who are predicted high who are low. Right away, we can tell that it is between our LASSO and our PCAmix models to use as our final model as LDA had a lower overall and recall accuracy compared to the other two. 

So between our LASSO and PCAmix models. Looking at both, they each have almost exactly the same recall percentage indicating that they both correctly predict a low student at the same rate. Yet our PCAmix model has a higher precision score of 40.8% as compared to our LASSO model, meaning that it didn't predict so many high students as low students. While this is not a major issue as those who are high being identified as low will only give them more help to do better, it is still important to look at it when deciding between the two due to the recall being so close. 

In conclusion, our final chosen model is the PCAmix model, as it has a very high recall that is very close to LASSO's, yet maintains a better precision value than the LASSO model. This model will give us the best chance at identifying low students correctly, provides very few false positives (predicted high but is low), and gives us a better predicted low when high percentage than all other models. 
