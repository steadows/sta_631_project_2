---
title: "Education Group Project 2"
author: "Vinnie Gambotto, Steve Meadows, Sai Sreyaa Krishna Mallemala"
date: "`r Sys.Date()`"
output: 
   html_document:
    theme: cosmo
    toc: yes
    toc_float:
      collapsed: true
    code_folding: hide
    number_sections: true
    css: styles.css
editor_options: 
  markdown: 
    wrap: 72

---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(rsample)
# Set CRAN mirror
options(repos = c(CRAN = "https://cloud.r-project.org"))

```

<h1>Predicting Educational Success</h1>

## Introduction

Understanding the factors that contribute to students’ academic success has
become increasingly important, especially as schools, families, and community
partners seek effective ways to support K–12 learners. In this project, we
analyze data from the Parent and Family Involvement (PFI) in Education Survey, a
nationally representative dataset collected by the U.S. Census Bureau, to
explore relationships between family engagement, school choice, and student
academic outcomes.

Our goal is to identify which aspects of family involvement—such as
participation in school activities, communication with teachers, assistance with
homework, and perceptions of school quality—are most strongly associated with
markers of academic success. Using a range of Generalized Linear Models (GLMs)
and classification tools, including logistic regression, multinomial regression,
Poisson regression, and Linear and Quadratic Discriminant Analysis (LDA/QDA), we
aim to build predictive models that highlight the most influential variables.

By applying these statistical learning methods through the tidymodels framework,
we address the broader research question:

**Which family and school-related factors best predict student academic success, and how can these insights guide effective support strategies for K–12 students?**

The final results will be presented in a concise report that summarizes both our
modeling process and our key findings. Ultimately, our analysis is designed to
provide actionable insights for GVSU’s K–12 Connect initiative, helping inform
recommendations that support students, empower families, and strengthen
school–family partnerships.

```{r loading and preprocessing, include=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(readxl)
library(dplyr)
library(glmnet)
library(tidymodels)
install.packages("pROC")
library(pROC)
library(broom)
```


# Data Loading and Preprocessing

```{r}
# Load and preprocess the data
df_2019 <- read_excel("pfi-data.xlsx", sheet = "curated 2019")
```

# Standardizing ALLGRADEX for 2019 data

```{r}
df_2019 <- df_2019 %>%
  mutate(
    ALLGRADEX_2019raw = ALLGRADEX,
    ALLGRADEX_std = case_when(
      ALLGRADEX %in% c(2, 3) ~ 0,                       # both K flavors → Kindergarten
      ALLGRADEX >= 4 & ALLGRADEX <= 15 ~ ALLGRADEX - 3, # 1st–12th → 1–12
      TRUE ~ NA_real_
    )
  )
```

# Filtering for high school students only

```{r}
df <- df_2019 %>%
  filter(ALLGRADEX_std >= 9 & ALLGRADEX_std <= 12)
```

# Handling target variable classes and missing values

```{r}
df <- df %>%
  # turn -1 and 5 into NA
  mutate(SEGRADES = na_if(SEGRADES, -1),
         SEGRADES = na_if(SEGRADES, 5)) %>%

  # drop rows where SEGRADES is missing
  filter(!is.na(SEGRADES)) %>%

  # create binary target: high vs low
  mutate(
    success = case_when(
      SEGRADES %in% c(1, 2) ~ "high",
      SEGRADES %in% c(3, 4) ~ "low"
    ),
    success = factor(success, levels = c("low", "high"))
  )

```

# Removing all unnecessary variables

```{r}
df <- df %>%
  select(-ZIPLOCL, -CDOBMM, -CDOBYY, -BASMID, -ALLGRADEX_std, -MOSTIMPT, -INTNUM, -ALLGRADEX_2019raw, -ALLGRADEX, -SEGRADES, - HHPARN19_BRD)
```


```{r}
df <- df %>%
  mutate(
    across(
      .cols = where(is.numeric),
      .fns = ~ na_if(.x , -1)
    )
  )
```

```{r}
df %>% filter(if_any(everything(), is.na))%>%
  tally()

```

```{r}
df<-df %>% drop_na()
```

```{r}
df %>% filter(if_any(everything(), is.na))%>%
  tally()
```

Separate our numerical variables from the rest of our data to make cleaning out the -1, 0, and 5 values with SEGRADES easier.
```{r}
num_vars <- c(
  "FHWKHRS", 
  "FSFREQ",
  "HHTOTALXX",
  "P1HRSWK",
  "P1MTHSWRK",
  "P1AGE",
  "NUMSIBSX"
)
```

```{r}
df <- df %>%
  mutate(
    across(
      .cols = where(is.numeric) & !all_of(num_vars),  
      .fns = ~ ifelse(.x < 0, 0, .x)                
    )
  )

```

Separate into our test and training set. We will use the training set to run our model. From there we will use the test data for our evaluation. We will then see how well our testing set does later on with this model.
```{r}
set.seed(1986)

data_split <- initial_split(df, prop = 0.8, strata = success)
train <- training(data_split)
test <- testing(data_split)
```

```{r}
train_x <- train %>% select(-success)
test_x  <- test  %>% select(-success)
train_x_matrix <- model.matrix(~ . - success - 1, data = train)
test_x_matrix  <- model.matrix(~ . - success - 1, data = test)

train_y <- train$success
test_y <- test$success

```

Build our LASSO model. 
```{r}
lasso.mod <- glmnet(
  x = train_x_matrix,
  y = train_y,
  alpha = 1, 
  family = "binomial"
)
```

```{r}
plot(lasso.mod)
title("LASSO Coefficient Paths")
```
Here we can see how each predictor’s coefficient changes as the penalty λ changes. It visualizes variable selection and shrinkage, letting you see which predictors enter the model and when. There are about 6 variables that enter the model early and the rest come around 4.5 for -Log(λ)
```{r}
set.seed(1986)

cv.lasso <- cv.glmnet(
  x = train_x_matrix,
  y = train_y,
  alpha = 1,
  family = "binomial",
  type.measure = "class"
)

plot(cv.lasso)
title("LASSO Cross-Validation Curve")

```
Above the plot is suggesting about 22 variables as we see the second dash line crossing right there with the MSE's. Right around that point, we have a -Log(λ) of between 4 and 5. 
```{r}
best_lambda <- cv.lasso$lambda.min
best_lambda

```

```{r}
lasso.final <- glmnet(
  x = train_x_matrix,
  y = train_y,
  alpha = 1,
  lambda = best_lambda,
  family = "binomial"
)

coef_final <- coef(lasso.final)
coef_final

```
This does work nicely with our MSE plot above as this also concludes with the idea of 22 variables. And the MSE of the 22 variables are better shown below in the table. 


```{r}
coef_final[coef_final != 0]

```

```{r}
lasso.prob <- predict(cv.lasso, newx = test_x_matrix, s = "lambda.min", type = "response")
lasso.pred <- ifelse(lasso.prob > 0.5, "high", "low") %>% factor(levels = c("low", "high"))
mean(lasso.pred == test_y)

```
Our training model gave us a 90.4% correct classification rate with our test data set, suggesting a very strong model that is consistent and reproducible. 
```{r}
table(Predicted = lasso.pred, Actual = test_y)

```

Our main priority is looking at our "false positives", meaning the students we predicted to be high and who are actually low. This is an issue because with our model if we predict someone to be getting high grades and they are actually getting low they would not be getting the help they need. We are not so worried about false negatives as those students are predicted low who are actually high will just get more help and will do better thus furthering themselves away from being predicted as low. 

```{r}
roc_obj <- roc(test_y, as.numeric(lasso.prob))
plot(roc_obj)
auc(roc_obj)

```
Finally, our AUC curve shows that 92.4% of the data is under the curve, showing a very effective model in predicting a students grades being low or high. 


```{r}
coef_df <- as.data.frame(as.matrix(coef_final)) %>%
  rownames_to_column("variable") %>%
  rename(coef = s0) %>%
  filter(coef != 0)
coef_df
```

